{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是Embedding层？\n",
    "\n",
    "在自然语言处理中，文本数据通常以词或字符的形式存在，而计算机无法直接处理这些离散的符号。因此，**Embedding层**的作用是将这些离散的词转换为连续的、高维的数值向量，这些向量通常能够捕捉词语之间的语义关系。\n",
    "\n",
    "### Embedding层的基本原理：\n",
    "1. **输入**: 一个整数序列（代表单词索引）。\n",
    "2. **映射**: 通过一个矩阵，将输入索引映射到一个固定大小的向量（查找矩阵的一行）。\n",
    "3. **输出**: 词向量，通常用于后续RNN层的输入。\n",
    "\n",
    "### 让我们手工实现一个Embedding层：\n",
    "\n",
    "#### 1. 依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 初始化Embedding矩阵\n",
    "假设我们有一个词汇表大小为 `vocab_size = 10`，每个词将被映射到一个维度为 `embedding_dim = 4` 的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbedding:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        # 随机初始化嵌入矩阵（大小：vocab_size x embedding_dim）\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, input_indices):\n",
    "        \"\"\"\n",
    "        input_indices: 一个包含词索引的数组，例如 [1, 3, 5]\n",
    "        返回: 嵌入向量的数组\n",
    "        \"\"\"\n",
    "        return self.embeddings[input_indices]\n",
    "    \n",
    "    def backward(self, input_indices, grad_output, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        执行反向传播：更新嵌入矩阵的权重。\n",
    "        \n",
    "        input_indices: 输入的单词索引列表\n",
    "        grad_output: 上游的梯度\n",
    "        learning_rate: 学习率\n",
    "        \"\"\"\n",
    "        for i, index in enumerate(input_indices):\n",
    "            self.embeddings[index] -= learning_rate * grad_output[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 测试Embedding层的前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引: [1 3 5]\n",
      "对应的嵌入向量:\n",
      " [[ 1.75698167  0.53049996  0.75759184 -1.86272847]\n",
      " [-1.06185763  1.02098985  0.89978505 -0.07378516]\n",
      " [-1.12500159  0.83426525 -0.83575991 -0.60213772]]\n"
     ]
    }
   ],
   "source": [
    "# 创建Embedding层\n",
    "embedding_layer = SimpleEmbedding(vocab_size=10, embedding_dim=4)\n",
    "\n",
    "# 假设输入是词索引序列 [1, 3, 5]\n",
    "input_indices = np.array([1, 3, 5])\n",
    "embedded_vectors = embedding_layer.forward(input_indices)\n",
    "\n",
    "print(\"输入索引:\", input_indices)\n",
    "print(\"对应的嵌入向量:\\n\", embedded_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 反向传播（模拟训练过程）\n",
    "假设从上层得到了损失函数的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新后的嵌入矩阵:\n",
      " [[ 1.45538239  0.5496268  -0.74792845  0.68092713]\n",
      " [ 1.75598167  0.52849996  0.75859184 -1.86322847]\n",
      " [ 1.74324464  0.93800379  0.32398963  0.17839578]\n",
      " [-1.06195763  1.02148985  0.89778505 -0.07278516]\n",
      " [-0.00582238  0.0865723  -0.46563895 -0.50950293]\n",
      " [-1.12400159  0.83376525 -0.83675991 -0.60013772]\n",
      " [ 1.74711793  0.43752603 -0.44210632 -0.42132947]\n",
      " [ 0.72313972  0.62019241  0.30399549  0.95957268]\n",
      " [ 1.21556613 -0.35346932 -1.07034161 -0.36901774]\n",
      " [ 0.66738462  0.47894538  1.35162036 -0.89991466]]\n"
     ]
    }
   ],
   "source": [
    "grad_output = np.array([\n",
    "    [0.1, 0.2, -0.1, 0.05],  # 对应第1个单词的梯度\n",
    "    [0.01, -0.05, 0.2, -0.1],  # 对应第3个单词的梯度\n",
    "    [-0.1, 0.05, 0.1, -0.2]   # 对应第5个单词的梯度\n",
    "])\n",
    "\n",
    "# 执行反向传播更新嵌入矩阵\n",
    "embedding_layer.backward(input_indices, grad_output, learning_rate=0.01)\n",
    "\n",
    "print(\"更新后的嵌入矩阵:\\n\", embedding_layer.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在来尝试处理一下文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如，把下面一首儿歌转换为词向量应该怎么做呢？\n",
    "\n",
    "> Mary had a little lamb,  \n",
    ">   Its fleece was white as snow.   \n",
    "> And everywhere that Mary went,  \n",
    ">   The lamb was sure to go.  \n",
    "> He followed her to school one day,  \n",
    ">   That was against the rule.  \n",
    "> It made the children laugh and play  \n",
    ">   To see a lamb at school.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 预处理文本**\n",
    "- 将文本转换为小写（或其他标准化）。\n",
    "- 去除标点符号、特殊字符。\n",
    "- 将文本拆分为单词（tokenization）。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 构建词汇表**\n",
    "- 生成一个单词索引映射（将每个唯一的单词分配一个整数索引）。\n",
    "- 确保索引唯一，通常使用字典数据结构。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 将文本序列化**\n",
    "- 使用词汇表将每个单词转换为索引，形成数字化的表示。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 应用Embedding层**\n",
    "- 使用预训练的嵌入（如GloVe、Word2Vec）或随机初始化的嵌入矩阵将索引转换为向量。\n",
    "\n",
    "---\n",
    "\n",
    "### **代码示例：从文本到嵌入向量**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['mary', 'had', 'a', 'little', 'lamb', 'its', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'mary', 'went', 'the', 'lamb', 'was', 'sure', 'to', 'go', 'he', 'followed', 'her', 'to', 'school', 'one', 'day', 'that', 'was', 'against', 'the', 'rule', 'it', 'made', 'the', 'children', 'laugh', 'and', 'play', 'to', 'see', 'a', 'lamb', 'at', 'school']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 1. 输入文本数据\n",
    "text = \"\"\"\n",
    "Mary had a little lamb,\n",
    "Its fleece was white as snow.\n",
    "And everywhere that Mary went,\n",
    "The lamb was sure to go.\n",
    "He followed her to school one day,\n",
    "That was against the rule.\n",
    "It made the children laugh and play\n",
    "To see a lamb at school.\n",
    "\"\"\"\n",
    "\n",
    "# 2. 文本预处理\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # 转小写\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # 去除标点符号\n",
    "    tokens = text.split()  # 拆分成单词\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess_text(text)\n",
    "print(\"Tokenized words:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index: {'a': 1, 'against': 2, 'and': 3, 'as': 4, 'at': 5, 'children': 6, 'day': 7, 'everywhere': 8, 'fleece': 9, 'followed': 10, 'go': 11, 'had': 12, 'he': 13, 'her': 14, 'it': 15, 'its': 16, 'lamb': 17, 'laugh': 18, 'little': 19, 'made': 20, 'mary': 21, 'one': 22, 'play': 23, 'rule': 24, 'school': 25, 'see': 26, 'snow': 27, 'sure': 28, 'that': 29, 'the': 30, 'to': 31, 'was': 32, 'went': 33, 'white': 34, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "# 3. 创建词汇表（手动构建词汇索引）\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(sorted(set(tokens)))}\n",
    "word_to_index['<PAD>'] = 0  # 添加填充标记\n",
    "print(\"\\nWord Index:\", word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text to sequence: [21, 12, 1, 19, 17, 16, 9, 32, 34, 4, 27, 3, 8, 29, 21, 33, 30, 17, 32, 28, 31, 11, 13, 10, 14, 31, 25, 22, 7, 29, 32, 2, 30, 24, 15, 20, 30, 6, 18, 3, 23, 31, 26, 1, 17, 5, 25]\n"
     ]
    }
   ],
   "source": [
    "# 4. 将文本转换为数字序列\n",
    "def text_to_sequence(tokens, word_to_index):\n",
    "    return [word_to_index[word] for word in tokens]\n",
    "\n",
    "sequences = text_to_sequence(tokens, word_to_index)\n",
    "print(\"\\nText to sequence:\", sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequences: tensor([21, 12,  1, 19, 17, 16,  9, 32, 34,  4, 27,  3,  8, 29, 21, 33, 30, 17,\n",
      "        32, 28, 31, 11, 13, 10, 14, 31, 25, 22,  7, 29, 32,  2, 30, 24, 15, 20,\n",
      "        30,  6, 18,  3, 23, 31, 26,  1, 17,  5, 25])\n"
     ]
    }
   ],
   "source": [
    "# 5. 对序列进行填充\n",
    "max_len = len(sequences)\n",
    "sequence_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "padded_sequence = torch.cat([sequence_tensor, torch.zeros(max_len - len(sequence_tensor), dtype=torch.long)])\n",
    "\n",
    "print(\"\\nPadded Sequences:\", padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding shape: torch.Size([47, 50])\n",
      "\n",
      "Embedding output for first word: [ 0.26433146 -0.19831406 -0.47902045 -0.45806032 -0.27450997 -0.33025104\n",
      " -1.7811464   0.1441443   0.80690134  1.2267246   0.40146875  0.3023453\n",
      " -0.1322729  -0.8056068  -0.32924327  1.3471342   0.7433598   0.26333496\n",
      "  0.04966741 -0.4080802  -0.11481339 -0.56656396 -1.1198696  -0.0962145\n",
      "  0.987879    0.18058605  0.22027072 -1.3456892   0.42864525  0.3974306\n",
      "  0.83492815  0.74255425 -2.505622   -0.32177886  0.5958665   1.602083\n",
      "  0.5208352  -0.06902823  0.11588498 -0.43345642  0.95750386  0.49879646\n",
      " -0.16514462 -1.2297944  -0.12269031  0.7225716   1.0030996  -0.35023418\n",
      " -0.05920502  0.20383798]\n"
     ]
    }
   ],
   "source": [
    "# 6. 创建嵌入层（词汇表大小 + 嵌入维度）\n",
    "vocab_size = len(word_to_index)  # 词汇表大小\n",
    "embedding_dim = 50  # 嵌入维度，例如50维向量\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 7. 获取嵌入结果\n",
    "embedded_result = embedding_layer(padded_sequence)\n",
    "\n",
    "print(\"\\nEmbedding shape:\", embedded_result.shape)\n",
    "print(\"\\nEmbedding output for first word:\", embedded_result[0].detach().numpy())  # 示例首个单词的嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **解释代码：**\n",
    "1. **预处理文本**\n",
    "   - 将文本转换为小写，并去除标点符号。\n",
    "   - 使用 `split()` 拆分单词。\n",
    "  \n",
    "2. **构建词汇表**\n",
    "   - 使用 `Tokenizer` 将单词映射为索引，例如 `{'mary': 1, 'had': 2, ...}`。\n",
    "\n",
    "3. **文本序列化**\n",
    "   - 将文本替换为对应的索引值，例如 `[[1, 2, 3, 4, 5]]`。\n",
    "\n",
    "4. **填充序列**\n",
    "   - 确保所有序列长度一致，以便于处理。\n",
    "\n",
    "5. **嵌入层**\n",
    "   - 使用 `Embedding(input_dim, output_dim)` 进行随机初始化（或加载预训练权重）。\n",
    "\n",
    "6. **输出嵌入**\n",
    "   - 通过 `model.predict` 提取嵌入向量，每个单词映射为固定大小的向量（例如50维）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **使用预训练嵌入（如GloVe）**\n",
    "如果希望使用预训练嵌入而不是随机初始化，可以加载 GloVe 词向量，如下：\n",
    "\n",
    "```python\n",
    "# 加载 GloVe 预训练嵌入\n",
    "embedding_index = {}\n",
    "with open('glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# 创建嵌入矩阵\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    vector = embedding_index.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector\n",
    "\n",
    "# 使用预训练嵌入层\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "```\n",
    "\n",
    "在这里，`trainable=False` 表示不对嵌入层进行微调，而是直接使用预训练的词向量。\n",
    "\n",
    "---\n",
    "\n",
    "### **输出示例：**\n",
    "```plaintext\n",
    "Tokenized words: ['mary', 'had', 'a', 'little', 'lamb', 'its', 'fleece', ...]\n",
    "\n",
    "Word Index: {'mary': 1, 'had': 2, 'a': 3, 'little': 4, 'lamb': 5, ...}\n",
    "\n",
    "Text to sequence: [[1, 2, 3, 4, 5, 6, 7, 8, ...]]\n",
    "\n",
    "Padded Sequences:\n",
    "[[ 1  2  3  4  5  6  7  8  ...]]\n",
    "\n",
    "Embedding shape: (1, 40, 50)\n",
    "\n",
    "Embedding output for first word: [-0.23  0.15  0.30  0.12 ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **总结：**\n",
    "1. **文本预处理** -> 将文本标准化、分词。\n",
    "2. **构建词汇表** -> 词到索引的映射。\n",
    "3. **序列化** -> 将文本转换为数值序列。\n",
    "4. **应用嵌入层** -> 通过嵌入矩阵获取向量。\n",
    "5. **选择训练策略**：\n",
    "   - 训练自己的嵌入。\n",
    "   - 使用预训练的词向量。\n",
    "\n",
    "---\n",
    "\n",
    "希望这个解释和代码示例能够帮助你理解和实现文本到词嵌入的过程！如果有任何疑问，欢迎继续提问。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
