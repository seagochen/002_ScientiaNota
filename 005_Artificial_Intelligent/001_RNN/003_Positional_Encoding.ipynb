{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. 位置编码（Positional Encoding）**\n",
    "---\n",
    "\n",
    "#### **问题背景**\n",
    "Transformer 模型不同于 RNN，不具备处理序列顺序的自然能力。它将输入序列视为一个整体并进行并行处理，因此需要一种机制来引入序列的位置信息，以帮助模型理解输入数据的顺序。\n",
    "\n",
    "#### **位置编码的定义**\n",
    "位置编码（Positional Encoding）是 Transformer 解决序列顺序问题的一种方法，它在输入序列的词向量上加上特定的编码，以提供位置信息。\n",
    "\n",
    "#### **常见的实现方式**\n",
    "1. **正弦和余弦位置编码（Sine & Cosine Positional Encoding）**  \n",
    "   - 在 Transformer 论文《Attention Is All You Need》中提出的方式。\n",
    "   - 位置编码的计算公式如下：\n",
    "     $$\n",
    "     PE_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{2i/d_{model}}})\n",
    "     $$\n",
    "     $$\n",
    "     PE_{(pos, 2i+1)} = \\cos(\\frac{pos}{10000^{2i/d_{model}}})\n",
    "     $$\n",
    "   - 其中：\n",
    "     - \\( pos \\) 是单词在序列中的位置。\n",
    "     - \\( i \\) 是编码维度的索引。\n",
    "     - \\( d_{model} \\) 是嵌入向量的维度。\n",
    "\n",
    "   代码示例（PyTorch）：\n",
    "   ```python\n",
    "   import torch\n",
    "   import math\n",
    "\n",
    "   def positional_encoding(seq_len, d_model):\n",
    "       pos = torch.arange(seq_len).unsqueeze(1)\n",
    "       i = torch.arange(d_model).unsqueeze(0)\n",
    "       angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)\n",
    "       pe = pos * angle_rates\n",
    "       pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "       pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "       return pe.unsqueeze(0)  # shape: (1, seq_len, d_model)\n",
    "\n",
    "   encoding = positional_encoding(50, 512)\n",
    "   print(encoding.shape)  # torch.Size([1, 50, 512])\n",
    "   ```\n",
    "\n",
    "2. **可学习的位置编码（Learnable Positional Encoding）**\n",
    "   - 作为模型参数进行学习，而非使用固定的正弦/余弦函数。\n",
    "   - 直接使用嵌入层（`nn.Embedding`）学习每个位置的权重。\n",
    "\n",
    "   ```python\n",
    "   import torch.nn as nn\n",
    "\n",
    "   class LearnablePositionalEncoding(nn.Module):\n",
    "       def __init__(self, max_len, d_model):\n",
    "           super().__init__()\n",
    "           self.encoding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "       def forward(self, x):\n",
    "           pos_ids = torch.arange(x.shape[1]).unsqueeze(0).to(x.device)\n",
    "           return self.encoding(pos_ids)\n",
    "\n",
    "   pos_enc = LearnablePositionalEncoding(100, 512)\n",
    "   print(pos_enc(torch.zeros(1, 10).long()).shape)  # torch.Size([1, 10, 512])\n",
    "   ```\n",
    "\n",
    "#### **作用**\n",
    "- 在 Transformer 中，位置编码会与输入的词嵌入（word embeddings）相加，以提供位置信息，帮助模型捕获序列顺序。\n",
    "- 由于正弦和余弦编码在维度上具有周期性，它可以在长序列上进行泛化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. 多头注意力机制（Multi-Head Attention）**\n",
    "---\n",
    "\n",
    "#### **问题背景**\n",
    "传统的注意力机制只能关注输入序列的某些部分，而 Transformer 使用 **多头注意力机制** 来提高模型的表示能力，增强不同部分之间的依赖关系。\n",
    "\n",
    "#### **多头注意力的定义**\n",
    "多头注意力机制的核心思想是：  \n",
    "将输入数据投影到多个不同的表示子空间，在这些子空间中分别计算注意力，然后将多个注意力结果合并。\n",
    "\n",
    "#### **计算步骤**\n",
    "1. **输入：**\n",
    "   - 词嵌入矩阵 \\( X \\in \\mathbb{R}^{seq\\_len \\times d\\_model} \\)\n",
    "\n",
    "2. **步骤：**\n",
    "   - 将输入分别线性映射为：\n",
    "     - 查询矩阵 \\( Q \\)\n",
    "     - 键矩阵 \\( K \\)\n",
    "     - 值矩阵 \\( V \\)\n",
    "\n",
    "   $$ Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V $$\n",
    "\n",
    "   - 计算注意力权重：\n",
    "     $$ Attention(Q, K, V) = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n",
    "\n",
    "   - 将多个注意力头的输出拼接，投影到最终输出维度。\n",
    "\n",
    "#### **多头注意力的优点**\n",
    "1. **增强表达能力：** 每个注意力头可以关注不同的语义信息（如短程关系、长程依赖）。\n",
    "2. **并行化处理：** 多个头同时运行，提高计算效率。\n",
    "3. **更好的特征捕获：** 避免单头注意力局限于特定维度。\n",
    "\n",
    "#### **数学公式**\n",
    "对于第 \\( i \\) 个注意力头：\n",
    "$$\n",
    "head_i = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i\n",
    "$$\n",
    "最终输出为：\n",
    "$$\n",
    "MultiHead(Q, K, V) = \\text{Concat}(head_1, head_2, ..., head_h) W^O\n",
    "$$\n",
    "\n",
    "#### **代码示例（PyTorch）**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0  # 确保可以整除\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 线性变换并分头 (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # 加权求和\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        return self.w_out(output)\n",
    "\n",
    "# 使用示例\n",
    "mha = MultiHeadAttention(512, 8)\n",
    "x = torch.rand(1, 10, 512)  # (batch_size, seq_len, d_model)\n",
    "out = mha(x, x, x)\n",
    "print(out.shape)  # torch.Size([1, 10, 512])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **总结**\n",
    "- **位置编码**：为输入序列提供位置信息，常见方法有正弦/余弦编码或可学习编码。\n",
    "- **多头注意力**：将输入投影到多个子空间，计算不同语义的注意力，提升模型的表达能力。\n",
    "\n",
    "如果你有任何具体问题或想要深入了解某个部分，欢迎继续讨论！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
