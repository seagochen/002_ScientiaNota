@[toc]

# 贝叶斯概率简述

在我写过的关于统计学相关文章 [《概率论基础 —— 2. 条件概率、全概率、贝叶斯概率公式》](https://seagochen.blog.csdn.net/article/details/118540995) 提到过一个很重要的概率公式—— 贝叶斯公式。其基本形式如下：

$$
P(x_i | Y) = \frac{P(x_i) P(Y | x_i)}{P(Y)} 
$$

这里的 $P(Y)$ 表示事件 $Y$ 发生的全概率，$P(x_i)$ 事件 $x_i$ 发生的概率，以及当 $Y$ 发生时的条件概率 $P(Y | x_i)$。

为了方便理解这个公式的概念，现在我们引入一个例子。假设某工厂有一条产品线，然后该产品线分别在该工厂的两个车间，或者两个生产线进行生产。由于车间的建设时间，以及车间生产线建设时间的不同，以及可能不同的安装工人对生产线的调试方法的不同，车间A和车间B都有不同程度的次品概率。

也就说我们可以用下表进行表示：

车间 | 产量占比| 良品率 | 次品率
--------|-------------|------------|-----
 A |  45% | 97% | 3%
 B | 55% | 95% | 5%

贝叶斯公式研究的问题就是，**当我们拿到了一件次品，它是车间A生产的概率和车间B生产的概率分别是多少**。

显然，你不能简单的用 3 /（3+5）来表示车间产自A次品的概率，所以我们对这类问题要使用一种全新的经验模型来评估这类事件的概率。

这就是贝叶斯概率模型。为了求解上面这个问题，首先我们要得到所有概率事件的全概率，因此有： 

$$
P(A)=\sum P(A | B) \cdot P(B)
$$

其中，A和B是随机事件，P(A|B) 表示B发生条件下A发生的概率，P(B)表示B发生的概率。其中 P(A|B) 又是条件概率，其公式表示为：

$$
p(A|B) = \frac{P(AB)}{P(B)}
$$

这里，P(AB)是事件A和事件B同时发生的概率。可以理解为在B发生的条件下，A发生的概率，又称为B条件下A发生的概率，因此相对以上这些知识点，我们可以获得当拿到一个次品时，它分别产自车间A的概率和车间B的概率为：

根据全概率公式，可以得到：

P(A|次品) = P(A and 次品) / P(次品)
P(B|次品) = P(B and 次品) / P(次品)

由于我们已知车间A和B的产量占比，因此可以得到：

P(次品) = P(A and 次品) + P(B and 次品)

所以，

P(A|次品) = P(A and 次品) / P(A and 次品) + P(B and 次品)
P(B|次品) = P(B and 次品) / P(A and 次品) + P(B and 次品)

P(A and 次品) = P(A) * P(次品|A) = 45% * 3% = 1.35%
P(B and 次品) = P(B) * P(次品|B) = 55% * 5% = 2.75%

即

P(A|次品) = 1.35 / (1.35 + 2.75) = 0.33
P(B|次品) = 2.75 / (1.35 + 2.75) = 0.67

因此，拿到一个次品时，它产自车间A的概率是33%，产自车间B的概率是67%。

显然，如果假设我们知道一个概率表的情况下，那么拿到某个样本时我们能很容易的通过贝叶斯模型计算出它可能的概率，因此在机器学习中，我们引入了一个新的分类神器，朴素贝叶斯。

# 朴素贝叶斯

朴素贝叶斯(Naive Bayes)是一种基于贝叶斯定理的分类算法。

贝叶斯定理：

P(A|B) = P(B|A) * P(A) / P(B)

其中，P(A|B)表示在已知B的条件下A的概率，P(B|A)表示在已知A的条件下B的概率，P(A)表示A的先验概率，P(B)表示B的先验概率。朴素贝叶斯算法基于贝叶斯定理，假设各特征之间相互独立，进行分类。

## 训练过程

首先对训练数据进行预处理，计算每个特征在每个类别中的概率。根据特征的独立性假设，计算每个类别的先验概率。保存计算得到的概率。


## 预测过程

对于新的输入样本，计算它属于每个类别的概率。

根据每个类别的先验概率和特征在类别中的概率计算出每个类别的后验概率。将输入样本分类到后验概率最大的类别中。

朴素贝叶斯算法的优点是简单易实现，时间复杂度低，可以处理大量数据，并且在线性分类器和贝叶斯分类器中表现较好。但是其假设特征之间相互独立，在实际应用中往往是不成立的，因此精度可能较低。

## 简单的说

看起来很复杂有没有，其实简单来说就是我们先设立一张表，然后把出现的数据类型A，B，C以及数据的属性a, b, c, d, e分别进行统计，然后得到每个样本，不同特征的概率，这就是先验概率。

类别 | 出现概率 | 特征a | 特征b | 特征c
-------|-----------------|------------|-----------|-----------
A | 25% |  40% | 30% | 30%
B | 30% | 10%  | 25% | 65%
C | 45% | 50% | 45% | 5%

然后对测试样本的特征进行分析，得到各特征出现的概率，然后再根据贝叶斯公式推算出它可能的分类。

这样说可能比较抽象，那么我们举垃圾邮件过滤机制为例，假设我们有数千封电子邮件作为样本，在我们完成对电子邮件样本分析和清洗后，得到类似一封邮件里如果出现类似重金求子，我有一笔存款与你分享等字样，那么大概率是垃圾邮件，而发现附件有Jpg文件但是文件大小有数十兆且没有具体说明的，有可能是木马邮件。

这样当你建立好这个过滤规则表后，并计算了先验概率。当你再次得到一封邮件时，你就可以对邮件内内容特征进行判断，给出它可能是有害邮件还是普通邮件的概率了。

程序示例，我准备放在下一章节里进行演示，有兴趣的朋友记得关注哟！


作为一种监督学习分类方法，在上一章中我们已经介绍过它的数理原理。现在我们开始来实现一个简单的朴素贝叶斯分类的算法，这样我们能更好的理解它是怎么运作的。

# 准备基础数据

首先还是有请我们的好朋友 numpy，让它给我们生成一些数据 。

```python
# 生成三类数据，每类数据有100个样本
np.random.seed(42)
class1 = np.random.randn(100, 2) * 0.5 + np.array([1, 1])
class2 = np.random.randn(100, 2) * 0.5 + np.array([-1, -1])
class3 = np.random.randn(100, 2) * 0.5 + np.array([1, -1])

# 将数据集分为训练集和测试集
X_train = np.concatenate([class1[:70], class2[:70], class3[:70]])
y_train = np.concatenate([np.zeros(70), np.ones(70), 2 * np.ones(70)])
X_test = np.concatenate([class1[70:], class2[70:], class3[70:]])
y_test = np.concatenate([np.zeros(30), np.ones(30), 2 * np.ones(30)])
```

这样我们的数据可以分为测试集和训练集两部分，这里我们给了全部的样本分别以标签1，2，3进行分类。

# 计算先验概率

在朴素贝叶斯分类器中，我们需要计算每个类别的先验概率。先验概率指的是在考虑任何特征的情况下，一个样本属于某个类别的概率。

先验概率的计算公式为：$P(Y=c_k)$，其中$Y$表示类别变量，$c_k$表示第$k$个类别。因为我们假设每个类别的样本数相等，所以可以用每个类别的样本数除以总样本数来估计先验概率。

具体到代码，计算先验概率的逻辑如下：

1. 首先创建一个大小为3的全零向量prior_probs，用于存储每个类别的先验概率；
2. 针对每个类别，使用Numpy的sum函数计算出训练集y_train中等于该类别的样本数量，并将其除以y_train的长度得到该类别的先验概率；
3. 将该类别的先验概率存储在prior_probs向量的相应位置上。
最终prior_probs向量中存储了每个类别的先验概率。

于是，我们可以得到下面这样的代码：

```python
prior_probs = np.zeros(3)
for i in range(3):
    prior_probs[i] = np.sum(y_train == i) / y_train.shape[0]
```

# 计算条件概率

条件概率指的是在已知一个样本属于某个类别的情况下，该样本的某个特征取某个值的概率。

具体到高斯朴素贝叶斯分类器中，我们假设每个特征在每个类别中的条件概率都是高斯分布。因此，我们需要计算每个类别中每个特征的均值和标准差，从而得到该特征在该类别下的高斯分布。这样，在预测时，我们就可以使用每个特征在每个类别下的高斯分布来计算该样本在该类别下的概率。

具体到代码，计算每个特征在每个类别中的条件概率的逻辑如下：

1. 首先创建两个大小为(3,特征数)的全零矩阵mean_vectors和std_vectors，用于存储每个类别中每个特征的均值和标准差；
2. 针对每个类别，使用Numpy的mean函数计算出训练集X_train中该类别下每个特征的均值，同时使用Numpy的std函数计算出该类别下每个特征的标准差；
3. 将该类别下每个特征的均值和标准差存储在mean_vectors和std_vectors矩阵的相应位置上。

最终，mean_vectors和std_vectors矩阵中存储了每个类别中每个特征的均值和标准差，用于计算条件概率。在预测时，我们将测试集中的每个样本输入到高斯分布中，计算出该样本在每个类别下的概率，并选择概率最大的类别作为预测结果。


```python
# 计算每个特征在每个类别中的条件概率
mean_vectors = np.zeros((3, X_train.shape[1]))
std_vectors = np.zeros((3, X_train.shape[1]))
for i in range(3):
    mean_vectors[i] = np.mean(X_train[y_train == i], axis=0)
    std_vectors[i] = np.std(X_train[y_train == i], axis=0)
```

# 预测分布

对于测试集中的每个样本，我们需要计算它在每个类别下的概率，从而得到该样本最可能属于哪个类别。在高斯朴素贝叶斯分类器中，我们假设每个特征在每个类别中的条件概率都是高斯分布，因此可以使用高斯分布的概率密度函数来计算每个特征值在每个类别中的条件概率。具体地，我们可以利用每个特征在每个类别下的均值和标准差，将该特征值输入到高斯分布的概率密度函数中，从而得到该特征值在该类别下的概率。最后，我们将每个特征的条件概率相乘，再乘以该类别的先验概率，得到该样本在该类别下的概率。在计算完每个类别下该样本的概率后，我们选择概率最大的类别作为该样本的预测标签。

1. 首先创建一个大小为y_test长度的全零向量y_pred，用于存储每个测试集样本的预测标签；
2. 针对每个测试集样本，使用Numpy的prod函数计算出该样本在每个类别下每个特征的条件概率，再将每个特征的条件概率相乘，乘以该类别的先验概率，得到该样本在每个类别下的概率；
3. 将该样本的预测标签设为概率最大的类别。

最终，y_pred向量中存储了每个测试集样本的预测标签，可以与真实标签进行比较以计算准确率。

```python
# 预测测试集的标签
y_pred = np.zeros(y_test.shape[0])
for i, x in enumerate(X_test):
    # 计算x在每个类别下的概率
    probs = np.zeros(3)
    for j in range(3):
        probs[j] = np.prod(1 / (np.sqrt(2 * np.pi) * std_vectors[j]) * np.exp(-(x - mean_vectors[j]) ** 2 / (2 * std_vectors[j] ** 2))) * prior_probs[j]
    y_pred[i] = np.argmax(probs)
```

# 验证结果
主要部分结束，我们看看输出的结果如何

![在这里插入图片描述](https://img-blog.csdnimg.cn/345002d299ac45a1a34b33a535852730.png#pic_center)
最终的准确度是93%，总体还是不错的。当然在现实中，我们要处理的特征肯定不止两维，这里只是一个简单的示例。
